{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from cleaning import convert      #function that turns data file into pandas dataframe\n",
    "#from cleaning import sized_export\n",
    "\n",
    "from langdetect import detect   #function that detects language of text\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)   #set seed for replicability (also applies to pandas functionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conversion Of Data into Pandas Dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import datasets and turn them into pandas dataframes using the convert function found in cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"C:/Users/Bijan-PC/Documents/Coding/UNIL/Data Analysis/ADA_Project/ADA_Final/dat_raw\"      #base location for where all raw data files are\n",
    "clean_root = \"C:/Users/Bijan-PC/Documents/Coding/UNIL/Data Analysis/ADA_Project/ADA_Final/dat_cleaned\"      #base location for where all cleaned data files will be saved\n",
    "\n",
    "twit_dta = convert(f\"{root}/emotions\",\"text.csv\")        #converts twitter csv file into pandas dataframe\n",
    "\n",
    "reddit_dta = convert(f\"{root}/reddit\", 'Reddit_Data.csv')\n",
    "\n",
    "yelp_dta_train = convert(f\"{root}/reviews/Yelp\",'train-00000-of-00001.parquet')         #yelp data came as pre-split, they will get merged in this notebook\n",
    "yelp_dta_test = convert(f\"{root}/reviews/Yelp\",'test-00000-of-00001.parquet')\n",
    "\n",
    "imdb_dta = convert(f\"{root}/reviews/IMDB\",'IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Twitter Posts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The emotions are classified into six categories: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_df = twit_dta.drop('Unnamed: 0', axis =1)          #drops column 'Unnamed: 0' which stored id for each twitter post\n",
    "twit_df['platform'] = 'Twitter'\n",
    "twit_df['data type'] = 'Social Media'                   #Adding columns to further describe data type and task\n",
    "twit_df['classification task'] = 'Multi-Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will balance this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    141067\n",
       "0    121187\n",
       "3     57317\n",
       "4     47712\n",
       "2     34554\n",
       "5     14972\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_df['label'].value_counts()      #Clearly unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bijan-PC\\AppData\\Local\\Temp\\ipykernel_26164\\3262325493.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  twit_df = pd.DataFrame(a.apply(lambda x: x.sample(a.size().min()).reset_index(drop=True)))\n"
     ]
    }
   ],
   "source": [
    "a = twit_df.groupby('label',group_keys=False)\n",
    "twit_df = pd.DataFrame(a.apply(lambda x: x.sample(a.size().min()).reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    14972\n",
       "1    14972\n",
       "2    14972\n",
       "3    14972\n",
       "4    14972\n",
       "5    14972\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_df['label'].value_counts()      #balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.23113144536468"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_df[\"text\"].apply(len).mean()   #Check average length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export balanced dataset in csv format\n",
    "twit_df.to_csv(clean_root + '/full/twit_clean.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Reddit Comments*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Dataset: \\\n",
    "0 Indicating a Neutral Comment \\\n",
    "1 Indicating a Postive comment \\\n",
    "-1 Indicating a Negative Comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_dta = reddit_dta.rename(columns={'clean_comment': 'text', 'category': 'label'})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_dta['label'] = (reddit_dta['label']+1)   #add one to each label. necessary for BERT as labels with negative values lead to issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_langue(text): #function to detect language          \n",
    "   try:\n",
    "       return detect(text)      #detect function imported from langdetect package\n",
    "   except:\n",
    "       return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_dta['language'] = reddit_dta['text'].apply(detect_langue) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_dta = reddit_dta[reddit_dta['language'] == 'en']            #keeps only english text\n",
    "reddit_dta = reddit_dta.drop('language', axis =1)               #drops language column\n",
    "\n",
    "reddit_dta['platform'] = \"Reddit\"            #Adds column Platform\n",
    "reddit_dta['data type'] = 'Social Media'         #Adds column Data Type\n",
    "reddit_dta['classification task'] = 'Multi-Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    14951\n",
       "1     8818\n",
       "0     7865\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_dta['label'].value_counts()      #Clearly unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bijan-PC\\AppData\\Local\\Temp\\ipykernel_26164\\3627001860.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  reddit_df = pd.DataFrame(a.apply(lambda x: x.sample(a.size().min()).reset_index(drop=True)))\n"
     ]
    }
   ],
   "source": [
    "a = reddit_dta.groupby('label',group_keys=False)\n",
    "reddit_df = pd.DataFrame(a.apply(lambda x: x.sample(a.size().min()).reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    7865\n",
       "1    7865\n",
       "2    7865\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['label'].value_counts()      #Now it's balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187.18198770926043"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df[\"text\"].apply(len).mean()   #Check average length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.to_csv(clean_root + '/full/reddit_clean.csv', index=False)  #export cleaned data to dat_cleaned/full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *IMDB Reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dta.loc[imdb_dta['sentiment'] == 'positive', 'label'] = 1      #Maps sentiment column (string) into binary values (int)\n",
    "imdb_dta.loc[imdb_dta['sentiment'] == 'negative', 'label'] = 0\n",
    "\n",
    "imdb_dta['label'] = imdb_dta['label'].astype(int)                   #Turns label float values into integers\n",
    "\n",
    "imdb_dta = imdb_dta.drop(['sentiment'], axis=1)                     #Drop sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dta = imdb_dta.rename(columns={'review': 'text'})             #rename review column to text\n",
    "imdb_dta['platform'] = \"IMDB\"            #Adds column platform\n",
    "imdb_dta['data type'] = 'Review'         #Adds column data Type\n",
    "imdb_dta['classification task'] = 'Binary'      #Adds column classification task\n",
    "imdb_df = imdb_dta[['text','label', 'platform', 'data type', 'classification task']]        #re-order columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB dataset is already balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    25000\n",
       "0    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309.43102"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df[\"text\"].apply(len).mean()   #Check average length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df.to_csv(clean_root + '/full/imdb_clean.csv', index=False)        #export cleaned data to dat_cleaned/full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Yelp Reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = pd.concat([yelp_dta_test, yelp_dta_train])                #combines pre-split train and test data into one dataset. Splitting will be done later\n",
    "yelp_df = yelp_df.reset_index(drop=True)                                      #restores indices post concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_langue(text): #function to detect language          \n",
    "   try:\n",
    "       return detect(text)      #detect function imported from langdetect package\n",
    "   except:\n",
    "       return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yelp_df['language'] =  yelp_lan.tolist()                       #can see what languages are present in dataset\n",
    "yelp_df['language'] = yelp_df['text'].apply(detect_langue)      #adds new language column to dataframe using detect_langue function above.\n",
    "yelp_df = yelp_df[yelp_df['language'] == 'en']            #keeps only english text\n",
    "yelp_df = yelp_df.drop('language', axis =1)               #drops language column\n",
    "\n",
    "yelp_df['platform'] = \"Yelp\"            #Adds column Platform\n",
    "yelp_df['data type'] = 'Review'         #Adds column Data Type\n",
    "yelp_df['classification task'] = 'Multi-Class'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df[['text','label','platform', 'data type', 'classification task']]                 #changes order of columns to the same as other dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    139213\n",
       "1    139050\n",
       "4    138681\n",
       "2    138655\n",
       "3    138477\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df['label'].value_counts()      #Clearly unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bijan-PC\\AppData\\Local\\Temp\\ipykernel_26164\\684707958.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  yelp_df = pd.DataFrame(a.apply(lambda x: x.sample(a.size().min()).reset_index(drop=True)))\n"
     ]
    }
   ],
   "source": [
    "a = yelp_df.groupby('label',group_keys=False)\n",
    "yelp_df = pd.DataFrame(a.apply(lambda x: x.sample(a.size().min()).reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    138477\n",
       "1    138477\n",
       "2    138477\n",
       "3    138477\n",
       "4    138477\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df['label'].value_counts()     #Now balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733.8679982957459"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df[\"text\"].apply(len).mean()   #Check average length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.to_csv(clean_root + '/full/yelp_clean.csv', index=False)  #export cleaned data to dat_cleaned/full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exporting different sized datasets for each platform**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the datasets are balanced, we can export different sized datasets. With the reddit dataset having the lowest number of observations for each label (7864), we will not exceed this number for any exports in order to make them comparable to each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7864 Observations (Large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I define the function sized_export here as well as in cleaning.py \\\n",
    "For some reason, when I use the imported verson for cleaning.py, the new csv file gets saved as \"None.csv\" but this issue doesn't occur when I define the function locally. I couldn't figure it out so I left it defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sized_export(df, size):\n",
    "    def get_var_name(var):                          #function for getting the name of a variable in string format\n",
    "        for name, value in globals().items():\n",
    "            if value is var:\n",
    "                return name\n",
    "    clean_root = \"C:/Users/Bijan-PC/Documents/Coding/UNIL/Data Analysis/ADA_Project/ADA_Final/dat_cleaned\"      #base location for where all cleaned data files are saved\n",
    "\n",
    "    df_name = get_var_name(df)\n",
    "\n",
    "    rows = df.groupby('label',group_keys=False)\n",
    "\n",
    "    new_df = pd.DataFrame(rows.apply(lambda x: x.sample(size, random_state = 2020).reset_index(drop=True)))\n",
    "    \n",
    "    ass = new_df['label'].value_counts()      #Clearly unbalanced\n",
    "\n",
    "    print(ass)\n",
    "    return new_df.to_csv(f\"{clean_root}/{size}\" + f'/{df_name}_{size}.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_large = 7852\n",
    "\n",
    "sized_export(twit_df, size_large)           #Exporting large csv files for the 4 data sets\n",
    "sized_export(reddit_df, size_large)\n",
    "sized_export(imdb_df, size_large)\n",
    "sized_export(yelp_df, size_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3500 Observations (Medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_medium = 3500\n",
    "\n",
    "sized_export(twit_df, size_medium)           #Exporting medium csv files for the 4 data sets\n",
    "sized_export(reddit_df, size_medium)    \n",
    "sized_export(imdb_df, size_medium)\n",
    "sized_export(yelp_df, size_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500 Observations (Small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_small = 500\n",
    "\n",
    "sized_export(twit_df, size_small)              #Exporting small csv files for the 4 data sets\n",
    "sized_export(reddit_df, size_small)\n",
    "sized_export(imdb_df, size_small)\n",
    "sized_export(yelp_df, size_small)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
