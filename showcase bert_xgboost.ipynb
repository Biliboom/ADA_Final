{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cleaning import convert\n",
    "from cleaning import splitter\n",
    "from cleaning import text_preprocessing\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500\n",
    "filename = \"imdb_df_500.csv\"\n",
    "root = f\"C:/Users/Bijan-PC/Documents/Coding/UNIL/Data Analysis/ADA_Project/ADA_Final/dat_cleaned/{size}\"      #base location for where all data files are\n",
    "df = convert(root,filename)            #Definining dataframe where all training and validation data will be pulled from\n",
    "df[\"id\"] = df.index                            #creating id column\n",
    "number_of_labels = len(df[\"label\"].value_counts())          #number of labels (will be used when training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def get_embeddings(texts: List[str], batch_size: int):\n",
    "    all_embeddings = []\n",
    "    print(f\"Total number of records: {len(texts)}\")\n",
    "    print(f\"Num batches: {(len(texts) // batch_size) + 1}\")\n",
    "\n",
    "    # Extract embeddings for the texts in batches\n",
    "    for start_index in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[start_index:start_index + batch_size]\n",
    "\n",
    "        # Generate tokens and move input tensors to GPU\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Extract the embeddings. no_grad because the gradient does not need to be computed\n",
    "        # since this is not a learning task\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get the last hidden stated and pool them into a mean vector calculated across the sequence length dimension\n",
    "        # This will reduce the output vector from [batch_size, sequence_length, hidden_layer_size]\n",
    "        # to [batch_size, hidden_layer_size] thereby generating the embeddings for all the sequences in the batch\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        embeddings = torch.mean(last_hidden_states, dim=1).cpu().tolist()\n",
    "\n",
    "        # Append to the embeddings list\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X = df.text.values\n",
    "y = df.label.values\n",
    "\n",
    "X_test, y_test, X_train, y_train, X_val, y_val = splitter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=['text'])\n",
    "X_val = pd.DataFrame(X_val, columns=['text'])\n",
    "X_test = pd.DataFrame(X_test, columns=['text'])\n",
    "\n",
    "y_train = pd.Series(y_train)\n",
    "y_test = pd.Series(y_test)\n",
    "y_val = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [X_test, X_train, X_val]:\n",
    "    dataset[\"text_cleaned\"] = dataset[\"text\"].apply(lambda x: text_preprocessing(x))\n",
    "    print(f'Cleaned {len(dataset[\"text_cleaned\"])} records in dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get class counts\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get embeddings for the training and test set\n",
    "train_embeddings = get_embeddings(texts=X_train[\"text_cleaned\"].tolist(), batch_size=256)\n",
    "train_embeddings_df = pd.DataFrame(train_embeddings)\n",
    "\n",
    "test_embeddings = get_embeddings(texts=X_test[\"text_cleaned\"].tolist(), batch_size=256)\n",
    "test_embeddings_df = pd.DataFrame(test_embeddings)\n",
    "\n",
    "val_embeddings = get_embeddings(texts=X_val[\"text_cleaned\"].tolist(), batch_size=256)\n",
    "val_embeddings_df = pd.DataFrame(val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    boost_device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not the cpu will be used\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    boost_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_model(data: pd.DataFrame, labels: pd.Series):\n",
    "\n",
    "    # Initialize the XGBoost Classifier\n",
    "    xgb_clf = xgb.XGBClassifier(objective=\"multi:softmax\",\n",
    "                                num_class = number_of_labels,\n",
    "                                device=boost_device,\n",
    "                                random_state=3137)\n",
    "\n",
    "    # Define hyperparameters and values to tune\n",
    "    param_grid = {\n",
    "        'max_depth': [5, 6, 7, 8],\n",
    "        'eta': np.arange(0.05, 0.3, 0.05)\n",
    "    }\n",
    "\n",
    "    print(f\"Number of rows in training data: {len(data)}\")\n",
    "\n",
    "    # Perform hyperparameter tuning using GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, scoring=\"roc_auc_ovo\",\n",
    "                               cv=5, verbose=3)\n",
    "    grid_search.fit(data, labels)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_max_depth = grid_search.best_params_['max_depth']\n",
    "    best_eta = grid_search.best_params_['eta']\n",
    "\n",
    "    final_xgb_clf = xgb.XGBClassifier(objective=\"multi:softmax\",\n",
    "                                      max_depth=best_max_depth,\n",
    "                                      eta=best_eta,\n",
    "                                      device=boost_device,\n",
    "                                      random_state=3137)\n",
    "    final_xgb_clf.fit(data, labels)\n",
    "\n",
    "    return final_xgb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(number_of_labels, number_of_labels))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) \n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False) \n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train model\n",
    "xgb_model = train_model(data=train_embeddings_df, labels=y_train)\n",
    "\n",
    "# Predict from model on validation data\n",
    "y_pred_val = xgb_model.predict(val_embeddings_df)\n",
    "#y_pred_labels = [label_dict[x] for x in y_pred]\n",
    "y_pred_val = pd.Series(y_pred_val)\n",
    "\n",
    "\n",
    "# Predict from model on test data\n",
    "y_pred_test = xgb_model.predict(test_embeddings_df)\n",
    "#y_pred_labels = [label_dict[x] for x in y_pred]\n",
    "y_pred_test = pd.Series(y_pred_test)\n",
    "\n",
    "# Evaluate model (validation data)\n",
    "print(f\"Classification report:\\n{classification_report(y_val, y_pred_val)}\")\n",
    "print(plot_confusion_matrix(y_val, y_pred_val))\n",
    "\n",
    "# Evaluate model (test data)\n",
    "print(f\"Classification report:\\n{classification_report(y_val, y_pred_test)}\")\n",
    "print(plot_confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_XGBOOST import bert_xgboost\n",
    "\n",
    "bert_xgboost('twit_df_500.csv', 500, test=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
